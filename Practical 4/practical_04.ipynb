{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "W_0 = np.random.randn(units[1], units[0] + 1) * 0.1\n",
    "W_1 = np.random.randn(units[2], units[1] + 1) * 0.1\n",
    "W_2 = np.random.randn(units[3], units[2] + 1) * 0.1\n",
    "\n",
    "input_w_bias = np.hstack((np.ones((inputs.shape[0], 1)), inputs)).T\n",
    "\n",
    "n = 0.1 \n",
    "\n",
    "phi = np.tanh\n",
    "phi_deriv = lambda x: 1 - np.tanh(x)**2\n",
    "\n",
    "print(input_w_bias.shape) \n",
    "print(W_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1 = W_0 @ input_w_bias\n",
    "h_1 = phi(a_1)\n",
    "\n",
    "h_1_bias = np.vstack((np.ones((1, h_1.shape[1])), h_1))\n",
    "\n",
    "a_2 = W_1 @  h_1_bias\n",
    "h_2 = phi(a_2)\n",
    "\n",
    "h_2_bias = np.vstack((np.ones((1, h_2.shape[1])), h_2))\n",
    "\n",
    "a_3 = W_2 @  h_2_bias\n",
    "h_3 = a_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean((h_3 - labels.T) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "dE_da3 = 2 * (h_3 - labels.T) / labels.shape[0]\n",
    "dE_dW2 = dE_da3 @ h_2_bias.T\n",
    "dE_dh2 = W_2[:, 1:].T @ dE_da3\n",
    "dE_da2 = dE_dh2 * phi_deriv(a_2)\n",
    "dE_dW1 = dE_da2 @ h_1_bias.T\n",
    "dE_dh1 = W_1[:, 1:].T @ dE_da2\n",
    "dE_da1 = dE_dh1 * phi_deriv(a_1)\n",
    "dE_dW0 = dE_da1 @ input_w_bias.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Weights\n",
    "W_2 -= n * dE_dW2\n",
    "W_1 -= n * dE_dW1\n",
    "W_0 -= n * dE_dW0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "phi_2 = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "cross_entropy_loss = lambda x : -np.dot(labels, np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_3 = phi_2(a_3)\n",
    "\n",
    "loss = cross_entropy_loss(h_3)\n",
    "\n",
    "dE_da3 = h_3 - labels.T\n",
    "dE_dW2 = dE_da3 @ h_2_bias.T\n",
    "dE_dh2 = W_2[:, 1:].T @ dE_da3\n",
    "dE_da2 = dE_dh2 * phi_deriv(a_2)\n",
    "dE_dW1 = dE_da2 @ h_1_bias.T\n",
    "dE_dh1 = W_1[:, 1:].T @ dE_da2\n",
    "dE_da1 = dE_dh1 * phi_deriv(a_1)\n",
    "dE_dW0 = dE_da1 @ input_w_bias.T\n",
    "\n",
    "# Update Weights\n",
    "W_2 -= n * dE_dW2\n",
    "W_1 -= n * dE_dW1\n",
    "W_0 -= n * dE_dW0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    a = []\n",
    "    hiddens = []\n",
    "\n",
    "    # compute hidden layers\n",
    "    for i in range(num_layers - 1):\n",
    "        # calulate a_i and h_i\n",
    "        if i == 0:\n",
    "            current_a = weights[i] @ x + biases[i]\n",
    "        else:\n",
    "            current_a = weights[i] @ hiddens[-1] + biases[i]\n",
    "        current_h = g(current_a)\n",
    "        a.append(current_a)\n",
    "        hiddens.append(current_h)\n",
    "\n",
    "    # compute output layer\n",
    "    output = weights[-1] @ hiddens[-1] + biases[-1]\n",
    "    \n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    # compute loss\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    loss = -np.dot(y, np.log(probs))\n",
    "    return loss   \n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    \n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    grad_z = probs - y  \n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations \n",
    "    for i in range(num_layers-1, -1, -1):\n",
    "        \n",
    "        # Gradient of hidden parameters.\n",
    "        if i == num_layers - 1:\n",
    "            h_prev = hiddens[i-1] if i > 0 else x\n",
    "            grad_W = np.outer(grad_z, h_prev)\n",
    "            grad_b = grad_z\n",
    "            grad_weights.append(grad_W)\n",
    "            grad_biases.append(grad_b)\n",
    "        else:\n",
    "            # Backpropagate to previous layer\n",
    "            grad_h = weights[i+1].T @ grad_z\n",
    "            grad_a = grad_h * (1 - hiddens[i] ** 2)  # Derivative of tanh\n",
    "            grad_z = grad_a\n",
    "            \n",
    "            h_prev = hiddens[i-1] if i > 0 else x\n",
    "            grad_W = np.outer(grad_z, h_prev)\n",
    "            grad_b = grad_z\n",
    "            grad_weights.append(grad_W)\n",
    "            grad_biases.append(grad_b)\n",
    "        \n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases, learning_rate):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0\n",
    "    \n",
    "    # For each observation and target\n",
    "    for i in range(inputs.shape[0]):\n",
    "        # Compute forward pass\n",
    "        output, hiddens = forward(inputs[i], weights, biases)\n",
    "        \n",
    "        # Compute Loss and update total loss\n",
    "        loss = compute_loss(output, labels[i])\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(inputs[i], labels[i], output, hiddens, weights)\n",
    "        \n",
    "        # Update weights\n",
    "        for j in range(num_layers):\n",
    "            weights[j] -= learning_rate * grad_weights[j]\n",
    "            biases[j] -= learning_rate * grad_biases[j]\n",
    "            \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 212.51754678958724\n",
      "Epoch 20, Loss: 96.72612458605529\n",
      "Epoch 30, Loss: 60.20860436210406\n",
      "Epoch 40, Loss: 43.50153010883951\n",
      "Epoch 50, Loss: 33.26274367083264\n",
      "Epoch 60, Loss: 26.494399359662385\n",
      "Epoch 70, Loss: 21.955099766717954\n",
      "Epoch 80, Loss: 18.727542683454306\n",
      "Epoch 90, Loss: 16.217304549198303\n",
      "Epoch 100, Loss: 14.253861463314193\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "units = [X_train.shape[1], 50, y_train_ohe.shape[1]]\n",
    "# Initialize weights\n",
    "weights = [np.random.randn(units[1], units[0]) * 0.1, \n",
    "           np.random.randn(units[2], units[1]) * 0.1]\n",
    "biases = [np.zeros((units[1],)), np.zeros((units[2],))]\n",
    "\n",
    "# Empty loss list\n",
    "loss_history = []\n",
    "\n",
    "# Learning rate.\n",
    "n = 0.001\n",
    "    \n",
    "# Run epochs and append loss to list\n",
    "for epoch in range(100):\n",
    "    weights, biases, epoch_loss = MLP_train_epoch(X_train, y_train_ohe, weights, biases, n)\n",
    "    loss_history.append(epoch_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "\n",
    "# Plot loss evolution\n",
    "# Plot loss evolution\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        output, _ = forward(x, weights, biases)\n",
    "        predicted_label = np.argmax(output)\n",
    "        predicted_labels.append(predicted_label)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = MLP_predict(X_train, weights, biases)\n",
    "y_pred_test = MLP_predict(X_test, weights, biases)\n",
    "\n",
    "train_acc = np.mean(y_pred_train == y_train)\n",
    "test_acc = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993041057759221\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
